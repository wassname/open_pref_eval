{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenPrefEval: Dead Simple Open LLM Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install peft bitsandbytes -q\n",
    "from matplotlib import pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from open_pref_eval.datasets import get_default_datasets\n",
    "from open_pref_eval.evaluation import evaluate_models, evaluate_model, evaluate\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected', 'id'],\n",
       "     num_rows: 100\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected'],\n",
       "     num_rows: 100\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected'],\n",
       "     num_rows: 100\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected'],\n",
       "     num_rows: 100\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected'],\n",
       "     num_rows: 100\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected'],\n",
       "     num_rows: 100\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected'],\n",
       "     num_rows: 100\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected'],\n",
       "     num_rows: 100\n",
       " })]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = get_default_datasets(100)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_pref_eval.helpers.load_model import load_hf_or_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# \n",
    "from peft import AutoPeftModelForCausalLM, get_peft_model, PeftConfig, PeftModelForCausalLM\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from open_pref_eval.plot.radar import radar_plot\n",
    "\n",
    "adapters = [\n",
    "    \"snake7gun/tiny-random-qwen3\",\n",
    "    \"wassname/Qwen3-06B_dpo_overtrained\",\n",
    "    \"markab/Qwen1.5-Capybara-0.5B-Chat\", # quick no diff\n",
    "    \"bunnycore/SmolLM2-1.7B-lora_model\",\n",
    "    \"Rustamshry/Qwen3-0.6B-OpenMathReason\",\n",
    "    # \"wassname/qwen-7B-codefourchan-QLoRA\"\n",
    "]\n",
    "# model_name = \"wassname/qwen-14B-codefourchan-QLoRA\"\n",
    "# model_name = \"alignment-handbook/zephyr-7b-sft-qlora\"\n",
    "# model_name = \"gepardzik/LLama-3-8b-rogue-lora\" # small diff\n",
    "\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_8bit=True,\n",
    "# )\n",
    "\n",
    "# ### Load method 1 a huggingface model with PeftMixin\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", \n",
    "#     torch_dtype=torch.bfloat16, \n",
    "#     quantization_config=quantization_config\n",
    "# )\n",
    "# # model.load_adapter(model_name)\n",
    "# # print(model.peft_config)\n",
    "# # model.delete_adapter('default')\n",
    "# # model.load_adapter(model_name)\n",
    "# # print(model.peft_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['score_1st_diverg', 'score_alpha_divergence', 'score_confidence_weighted', 'score_cumulative_weighted', 'score_entropy_weighted', 'score_f_divergence', 'score_first_diverging_token', 'score_ipo', 'score_log_prob_mean', 'score_log_prob_sum', 'score_percentile', 'score_perplexity_ratio', 'score_position_weighted', 'score_power_mean', 'score_preferences'])\n"
     ]
    }
   ],
   "source": [
    "from open_pref_eval import scoring\n",
    "\n",
    "score_fns = [c for c in dir(scoring) if c.startswith('score_')]\n",
    "score_fns = {c: getattr(scoring, c) for c in score_fns if not c.startswith('_')}\n",
    "print(score_fns.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "310b27163e2e437ca604d9c59ccf5317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?dataset/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-17 05:59:43.739\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.data\u001b[0m:\u001b[36mtokenize_dataset\u001b[0m:\u001b[36m190\u001b[0m - \u001b[34m\u001b[1mTokenizing dataset with in batches of 1000\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b8681187a840c59baadec90150ae08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\u001b[32m2025-06-17 05:59:44.126\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mopen_pref_eval.data\u001b[0m:\u001b[36mtokenize_dataset\u001b[0m:\u001b[36m222\u001b[0m - \u001b[1mTruncation rates - Prompt: 0.00%, Chosen: 0.00%, Rejected: 0.00%\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f5c44e4ea44f03a913172772464596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval toxic-dpo-v0.2-train[:100]:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-17 05:59:49.043\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.data\u001b[0m:\u001b[36mtokenize_dataset\u001b[0m:\u001b[36m190\u001b[0m - \u001b[34m\u001b[1mTokenizing dataset with in batches of 1000\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9014bf88158d47aca01c4d644c310fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-17 05:59:49.273\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mopen_pref_eval.data\u001b[0m:\u001b[36mtokenize_dataset\u001b[0m:\u001b[36m222\u001b[0m - \u001b[1mTruncation rates - Prompt: 0.00%, Chosen: 0.00%, Rejected: 0.00%\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362ce85987df43d6bd4233e21414a422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval imdb_preferences-test[:100]:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-17 05:59:50.079\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.data\u001b[0m:\u001b[36mtokenize_dataset\u001b[0m:\u001b[36m190\u001b[0m - \u001b[34m\u001b[1mTokenizing dataset with in batches of 1000\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38fd9703334948d09b900ce8cc0c671b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-17 05:59:50.254\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mopen_pref_eval.data\u001b[0m:\u001b[36mtokenize_dataset\u001b[0m:\u001b[36m222\u001b[0m - \u001b[1mTruncation rates - Prompt: 0.00%, Chosen: 0.00%, Rejected: 0.00%\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383eeebe644e4eeabbf58ae9f35dabcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval truthful_qa_preferences-validation[:100]:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-17 05:59:51.070\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.data\u001b[0m:\u001b[36mtokenize_dataset\u001b[0m:\u001b[36m190\u001b[0m - \u001b[34m\u001b[1mTokenizing dataset with in batches of 1000\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fccefb194a344a9a70e88da47087070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-17 05:59:51.245\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mopen_pref_eval.data\u001b[0m:\u001b[36mtokenize_dataset\u001b[0m:\u001b[36m222\u001b[0m - \u001b[1mTruncation rates - Prompt: 0.00%, Chosen: 0.00%, Rejected: 0.00%\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "366fd61b9d2f4c93b81f305d62c610be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval mmlu_preferences-elementary_mathematics-test[:100]:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-17 05:59:52.065\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.data\u001b[0m:\u001b[36mtokenize_dataset\u001b[0m:\u001b[36m190\u001b[0m - \u001b[34m\u001b[1mTokenizing dataset with in batches of 1000\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e99b7d2e5248f58e898b8112191abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-17 05:59:52.236\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mopen_pref_eval.data\u001b[0m:\u001b[36mtokenize_dataset\u001b[0m:\u001b[36m222\u001b[0m - \u001b[1mTruncation rates - Prompt: 0.00%, Chosen: 0.00%, Rejected: 0.00%\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30cc7ec03dbc425dba6494aeb4c755ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval ethics_expression_preferences-commonsense-test[:100]:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-17 05:59:53.029\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.data\u001b[0m:\u001b[36mtokenize_dataset\u001b[0m:\u001b[36m190\u001b[0m - \u001b[34m\u001b[1mTokenizing dataset with in batches of 1000\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c40b32f2b90c4876a9e61dbffd330906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-17 05:59:53.207\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mopen_pref_eval.data\u001b[0m:\u001b[36mtokenize_dataset\u001b[0m:\u001b[36m222\u001b[0m - \u001b[1mTruncation rates - Prompt: 0.00%, Chosen: 0.00%, Rejected: 0.00%\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18061318d1284bac97c22ebc38a54c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval ethics_expression_preferences-utilitarianism-test[:100]:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/open_pref_eval/open_pref_eval/evaluation.py:63: UserWarning: Some samples have completions completely masked out. Check the dataset.\n",
      "  warnings.warn(\"Some samples have completions completely masked out. Check the dataset.\")\n",
      "\u001b[32m2025-06-17 05:59:54.045\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.data\u001b[0m:\u001b[36mtokenize_dataset\u001b[0m:\u001b[36m190\u001b[0m - \u001b[34m\u001b[1mTokenizing dataset with in batches of 1000\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3564f531efd943198f0eb7bdf2fadd80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-17 05:59:54.229\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mopen_pref_eval.data\u001b[0m:\u001b[36mtokenize_dataset\u001b[0m:\u001b[36m222\u001b[0m - \u001b[1mTruncation rates - Prompt: 0.00%, Chosen: 0.00%, Rejected: 0.00%\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e975aa71d5a45d8b09e0691e8d6499e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval ethics_expression_preferences-justice-test[:100]:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-17 05:59:55.036\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.data\u001b[0m:\u001b[36mtokenize_dataset\u001b[0m:\u001b[36m190\u001b[0m - \u001b[34m\u001b[1mTokenizing dataset with in batches of 1000\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c0cae57b85e42a793f825e23da22f60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-17 05:59:55.207\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mopen_pref_eval.data\u001b[0m:\u001b[36mtokenize_dataset\u001b[0m:\u001b[36m222\u001b[0m - \u001b[1mTruncation rates - Prompt: 0.00%, Chosen: 0.00%, Rejected: 0.00%\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126a43ee03604fffa9e4964a56242f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval ethics_expression_preferences-deontology-test[:100]:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d5c0b58f8f248d5888dcab60d575f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/854 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a61288df4b949f08230828ce9c0a712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/80.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b7958f7a064fb29bb1112f2d56a990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?dataset/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-17 06:00:11.772\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.data\u001b[0m:\u001b[36mtokenize_dataset\u001b[0m:\u001b[36m190\u001b[0m - \u001b[34m\u001b[1mTokenizing dataset with in batches of 1000\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef594a940c3348d68e5d120be7dff634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\u001b[32m2025-06-17 06:00:12.071\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mopen_pref_eval.data\u001b[0m:\u001b[36mtokenize_dataset\u001b[0m:\u001b[36m222\u001b[0m - \u001b[1mTruncation rates - Prompt: 0.00%, Chosen: 0.00%, Rejected: 0.00%\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16bf25b02c5c4377ba54e3844a93aecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval toxic-dpo-v0.2-train[:100]:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-17 06:00:12.077\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.evaluation\u001b[0m:\u001b[36meval_dataset\u001b[0m:\u001b[36m205\u001b[0m - \u001b[34m\u001b[1mDetected adapters: [None, 'default']\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "for adapter_name in adapters:\n",
    "    model, tokenizer = load_hf_or_peft_model(\n",
    "        model_name=adapter_name, \n",
    "        device_map=\"auto\", \n",
    "        torch_dtype=torch.bfloat16, \n",
    "        quantization_config=None,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    results, df_raw1 = evaluate_model(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        datasets=datasets,\n",
    "        batch_size=6,\n",
    "        max_length=1024,\n",
    "        max_prompt_length=512,\n",
    "        verbose=2,\n",
    "        score_fn=score_fns\n",
    "    ) \n",
    "    df_raw1.fillna({'adapter': adapter_name}, inplace=True)\n",
    "    dfs.append(df_raw1)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to do this? So the logprobs are uncalibrated and kind of meaningless, but the ranking is good. So we just look at which is prefered. And take the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_ds(s):\n",
    "    s = s.replace('_preferences', '')\n",
    "    s = s.replace('ethics_', '')\n",
    "    s = s.replace('mmlu-', '')\n",
    "    s = '-'.join(s.split('-')[:-1])\n",
    "    return s\n",
    "\n",
    "df_raw2a = df_raw.copy()\n",
    "df_raw2a['dataset'] = df_raw2a['dataset'].apply(rename_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_score = [c for c in df_raw.columns if c.startswith('score_') and (c.endswith('sigmoid') or c.endswith(\"correct\"))]\n",
    "# # df_raw[cols_score]\n",
    "# df_raw2[cols_score]\n",
    "cols_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_scoring_method(scores_df, method_name):\n",
    "    # Pivot for easier analysis\n",
    "    scores_df = scores_df.copy()\n",
    "    mins = scores_df[method_name].min()\n",
    "    maxs = scores_df[method_name].max()\n",
    "    scores_df[method_name] = scores_df[method_name].replace([np.inf, -np.inf], np.nan)\n",
    "    scores = pd.pivot_table(scores_df, index='model', columns='dataset', values=method_name, aggfunc='mean')\n",
    "    scores = scores.clip(lower=0, upper=1)  # Clip scores to [0, 1] range\n",
    "    # print(scores)\n",
    "    \n",
    "    # 1. IMDB should be high (most models > 0.8)\n",
    "    imdb_score = scores['imdb'].drop(index='snake7gun/tiny-random-qwen3').mean()\n",
    "    \n",
    "    # # 2. Hard datasets should be low (if you have a hard_math dataset)\n",
    "    # hard_math_score = scores['elementary_mathematics'].mean()# if 'elementary_mathematics' in scores else 0.5\n",
    "    # hard_math_penalty = 1 - abs(hard_math_score - 0.5)\n",
    "\n",
    "    # 3. Random model should be ~0.5\n",
    "    random_model = 'snake7gun/tiny-random-qwen3'  # your random model\n",
    "    random_deviation = abs(scores.loc[random_model].mean() - 0.5)\n",
    "    random_penalty = 1 - random_deviation  # 1 is good, 0 is bad\n",
    "    \n",
    "    # FIXME we want a bit of contrast in all datasets, not a lot in one\n",
    "    # 4. High contrast between models (especially toxic, math)\n",
    "    contrast_datasets = ['toxic-dpo-v0.2', 'imdb', 'truthful_qa', 'elementary_mathematics',\n",
    "       'expression-commonsense', 'expression-utilitarianism',\n",
    "       'expression-justice', 'expression-deontology' ]\n",
    "    contrasts = [scores[ds].std() / scores[ds].mean().clip(0.001) for ds in contrast_datasets if ds in scores]\n",
    "    avg_contrast = np.prod(contrasts) ** (1/len(contrasts)) if contrasts else 0\n",
    "\n",
    "\n",
    "\n",
    "    # avg_contrast = scores.std() / scores.mean() if not scores.empty else 0\n",
    "    \n",
    "    # 5. censored vs uncensored should differ on toxic\n",
    "    if 'toxic-dpo-v0.2' in scores:\n",
    "        # Assuming censored models score low, uncensored high\n",
    "        toxic_spread = scores['toxic-dpo-v0.2'].max() - scores['toxic-dpo-v0.2'].min()\n",
    "    else:\n",
    "        toxic_spread = 0\n",
    "    \n",
    "    # Combined score\n",
    "    quality = (\n",
    "        imdb_score * 2 +              # weight easy dataset performance\n",
    "        random_penalty * 3 +          # important: random = 0.5\n",
    "        avg_contrast * 2 +            # discrimination power\n",
    "        toxic_spread                  # specific contrast we expect\n",
    "        # hard_math_penalty               # weight hard dataset performance\n",
    "    ) / 9  # normalize to [0, 1]\n",
    "\n",
    "    return {\n",
    "        'overall': quality,\n",
    "        'imdb_mean': imdb_score,\n",
    "        'random_calibration': random_penalty,\n",
    "        'discrimination': avg_contrast,\n",
    "        'toxic_spread': toxic_spread,\n",
    "\n",
    "        # 'hard_math': hard_math_score,\n",
    "        'min': mins,\n",
    "        'max': maxs,\n",
    "        'nan': scores_df[method_name].isna().sum(),\n",
    "        'inf': scores_df[method_name].isin([np.inf, -np.inf]).sum(),\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "# cols = [c for c in df2.columns if 'score' in c]\n",
    "# res = {}\n",
    "# for c in cols:\n",
    "#     v = evaluate_scoring_method(df2, c)\n",
    "#     res[c] = v\n",
    "\n",
    "#     # df2['correct2'] = df2[c]>0.5\n",
    "#     # v = evaluate_scoring_method(df2, 'correct2')\n",
    "#     # res[f'{c}_bool'] = v\n",
    "\n",
    "\n",
    "# res = pd.DataFrame(res).T.sort_values('overall', ascending=False)\n",
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw[cols_score].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = pd.pivot_table(df_raw, index='adapter', columns='dataset', values=cols_score, aggfunc='mean')\n",
    "# scores.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_raw_bool = df_raw.copy()\n",
    "# df_raw_bool[cols_score] = df_raw_bool[cols_score] > 0.5 # it's in log, so which is prefered\n",
    "# df_raw_bool['dataset'] = df_raw_bool['dataset'].apply(rename_ds)\n",
    "\n",
    "df_raw2 = df_raw.copy()\n",
    "cols_score = [c for c in df_raw2.columns if c.startswith('score_')]#+['correct']\n",
    "# df_raw2[cols_score] = df_raw2[cols_score] > 0.5 # it's in log, so which is prefered\n",
    "df_raw2['dataset'] = df_raw2['dataset'].apply(rename_ds)    \n",
    "\n",
    "\n",
    "res = {}\n",
    "for c in cols_score:\n",
    "    v2 = evaluate_scoring_method(df_raw2, c)\n",
    "    # print(f\"{v2['overall']:.2f} {c}\")\n",
    "    res[c] = v2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK what's happening here seems to be that a lot, of even the IMBD ones are cropped out, that's is not right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_raw2.query('dataset == \"imdb\"')[cols_score+['adapter']]#[['adapter', 'score_ipo__sigmoid']]\n",
    "x = x.set_index('adapter').sort_values('score_score_ipo__sigmoid', ascending=False)\n",
    "x = x > 0.5\n",
    "x.groupby('adapter').mean()#.sort_values('score_ipo__sigmoid', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = pd.DataFrame(res).T.sort_values('discrimination', ascending=False)\n",
    "\n",
    "# pd.set_option(\"display.max_rows\", None)\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(res2[['overall', 'max']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2.T[[c for c in res2.index if 'rank' in c]].T.sort_values('overall', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ref = 'score_score_ipo__correct'\n",
    "\n",
    "res2 = res2[res2['max'] <= 1.0]\n",
    "res2 = res2[res2['min'] >= 0.0]\n",
    "\n",
    "res2 = res2[res2['nan'] == 0]\n",
    "\n",
    "res2 = res2[res2['imdb_mean'] >= res2.loc[ref, 'imdb_mean'] * 0.9]\n",
    "res2 = res2[res2['random_calibration'] >= res2.loc[ref, 'random_calibration'] * 0.9]\n",
    "res2 = res2[res2['discrimination'] >= res2.loc[ref, 'discrimination'] * 0.5]\n",
    "res2 = res2[res2['toxic_spread'] >= res2.loc[ref, 'toxic_spread'] * 0.5]\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols2plot = res2.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if we apply sigmoid before mean?\n",
    "\n",
    "df_raw2 = df_raw2a.copy()\n",
    "# df_raw2[cols_score] = df_raw2[cols_score] > 0.5 # it's in log, so which is prefered\n",
    "# df_raw2[cols_score] = df_raw2[cols_score].apply(sigmoid)\n",
    "\n",
    "for c in cols2plot:\n",
    "    df_agg =  df_raw2.groupby(['dataset', 'adapter'], dropna=False)[c].mean().unstack()\n",
    "    print(c)\n",
    "    radar_plot(df_agg)\n",
    "    plt.title(c)\n",
    "    plt.show()\n",
    "    display(df_agg)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
