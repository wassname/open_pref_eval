{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenPrefEval: Dead Simple Open LLM Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install peft bitsandbytes -q\n",
    "from matplotlib import pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from open_pref_eval.datasets import get_default_datasets\n",
    "from open_pref_eval.evaluation import evaluate_models, evaluate_model, evaluate\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected', 'id'],\n",
       "     num_rows: 100\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected'],\n",
       "     num_rows: 100\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected'],\n",
       "     num_rows: 100\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected'],\n",
       "     num_rows: 100\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected'],\n",
       "     num_rows: 100\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected'],\n",
       "     num_rows: 100\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected'],\n",
       "     num_rows: 100\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected'],\n",
       "     num_rows: 100\n",
       " })]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = get_default_datasets(100)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bef743df04c493b9d9c07f83d4f0c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# \n",
    "from peft import AutoPeftModelForCausalLM, get_peft_model, PeftConfig, PeftModelForCausalLM\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from open_pref_eval.plot.radar import radar_plot\n",
    "model_name = \"markab/Qwen1.5-Capybara-0.5B-Chat\" # quick no diff\n",
    "model_name = \"Rustamshry/Qwen3-0.6B-OpenMathReason\"\n",
    "\n",
    "\n",
    "model_name = \"wassname/qwen-7B-codefourchan-QLoRA\"\n",
    "# model_name = \"wassname/qwen-14B-codefourchan-QLoRA\"\n",
    "\n",
    "# model_name = \"alignment-handbook/zephyr-7b-sft-qlora\"\n",
    "# model_name = \"gepardzik/LLama-3-8b-rogue-lora\" # small diff\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "### Load method 1 a huggingface model with PeftMixin\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "# model.load_adapter(model_name)\n",
    "# print(model.peft_config)\n",
    "# model.delete_adapter('default')\n",
    "# model.load_adapter(model_name)\n",
    "# print(model.peft_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # ## Load method 2 a PeftModel\n",
    "# model = PeftModelForCausalLM.from_pretrained(\n",
    "#     model,\n",
    "#     model_name,\n",
    "#     device_map=\"auto\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     quantization_config=quantization_config,\n",
    "# )\n",
    "# print(model.peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model.peft_config['default'].base_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.delete_adapter('default')\n",
    "# model.load_adapter(model_name, '4chan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'default': LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='unsloth/Qwen2.5-Coder-7B-Instruct', revision=None, inference_mode=True, r=64, target_modules={'v_proj', 'gate_proj', 'down_proj', 'up_proj', 'k_proj', 'o_proj', 'q_proj'}, exclude_modules=None, lora_alpha=64, lora_dropout=0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)}\n"
     ]
    }
   ],
   "source": [
    "print(model.peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['score_1st_diverg', 'score_alpha_divergence', 'score_confidence_weighted', 'score_cumulative_weighted', 'score_entropy_weighted', 'score_f_divergence', 'score_first_diverging_token', 'score_information_weighted', 'score_ipo', 'score_log_prob_mean', 'score_log_prob_sum', 'score_percentile', 'score_perplexity_ratio', 'score_position_weighted', 'score_power_mean', 'score_precision_weighted', 'score_preferences', 'score_rank_based', 'score_with_vocab_uncertainty'])\n"
     ]
    }
   ],
   "source": [
    "from open_pref_eval import scoring\n",
    "\n",
    "score_fns = [c for c in dir(scoring) if c.startswith('score_')]\n",
    "score_fns = {c: getattr(scoring, c) for c in score_fns if not c.startswith('_')}\n",
    "print(score_fns.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from open_pref_eval.scoring import score_ipo, score_preferences, score_confidence_weighted, score_with_entropy_weight, score_seq_entropy_weighted, score_rank, score_cumsum, score_agg, score_power_mean, score_with_decay, score_percentile, score_weighted_prob, score_with_weight, score_f_alpha_divergance\n",
    "# score_fns = {\n",
    "#     \"ipo\": score_ipo,\n",
    "#     \"rank\": score_rank,\n",
    "#     \"preferences\": score_preferences,\n",
    "#     'confidence_weighted': score_confidence_weighted,\n",
    "#     'with_entropy_weight': score_with_entropy_weight,\n",
    "#     \"cumsum\": score_cumsum,\n",
    "#     # \"weighted\": score_weighted, # littlediff\n",
    "#     \"weighted_prob\": score_weighted_prob, # nan\n",
    "#     \"with_weight\": score_with_weight,\n",
    "#     \"f_alpha_divergance\": score_f_alpha_divergance,\n",
    "#     # \"f_divergance\": score_f_divergance,\n",
    "#     # \"min\": lambda *args, **kwargs: score_agg(*args, **kwargs, agg=lambda x: torch.min(x, dim=-1).values),\n",
    "#     # \"max\": lambda *args, **kwargs: score_agg(*args, **kwargs, agg=lambda x: torch.max(x, dim=-1).values),\n",
    "#     \"mean\": lambda *args, **kwargs: score_agg(*args, **kwargs, agg=lambda x: torch.mean(x, dim=-1)),\n",
    "#     \"median\": lambda *args, **kwargs: score_agg(*args, **kwargs, agg=lambda x: torch.median(x, dim=-1).values),\n",
    "#     \"std\": lambda *args, **kwargs: score_agg(*args, **kwargs, agg=lambda x: torch.std(x, dim=-1)),\n",
    "#     \"seq_entropy_weighted\": score_seq_entropy_weighted,\n",
    "#     # \"certainty_weighted\": score_certainty_weighted,\n",
    "#     \"power_mean\": score_power_mean,\n",
    "#     \"with_decay\": score_with_decay,\n",
    "#     \"percentile\": score_percentile,\n",
    "#     # \"1st_diverg\": score_1st_diverg,\n",
    "#     # \"perplexity_ratio\": score_perplexity_ratio,\n",
    "#     'confidence_weighted0.5': partial(score_confidence_weighted, T=0.5),\n",
    "#     'confidence_weighted5': partial(score_confidence_weighted, T=5.0),\n",
    "#     'confidence_weighted10': partial(score_confidence_weighted, T=10.0),\n",
    "#     'with_entropy_weight0.5': partial(score_with_entropy_weight, alpha=0.5),\n",
    "#     'with_entropy_weight5': partial(score_with_entropy_weight, alpha=5.0),\n",
    "#     'with_entropy_weight10': partial(score_with_entropy_weight, alpha=10.0),\n",
    "\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ab41ec0e4c4dbaa485432480a16e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?dataset/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a850afa28314275bda51aca421b5825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval toxic-dpo-v0.2-train[:100]:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-25 13:23:29.629\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.evaluation\u001b[0m:\u001b[36meval_dataset\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mDetected adapters: [None, 'default']\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709fd10d804e4d1d9e2b67570d052850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval imdb_preferences-test[:100]:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-25 13:24:33.765\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.evaluation\u001b[0m:\u001b[36meval_dataset\u001b[0m:\u001b[36m203\u001b[0m - \u001b[34m\u001b[1mDetected adapters: [None, 'default']\u001b[0m\n",
      "/media/wassname/SGIronWolf/projects5/elk/open_pref_eval/.venv/lib/python3.11/site-packages/transformers/integrations/peft.py:443: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n",
      "/media/wassname/SGIronWolf/projects5/elk/open_pref_eval/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[32m2025-05-25 13:24:38.570\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m85\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-25 13:24:42.743\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m85\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 33.33% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-25 13:24:52.423\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m85\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-25 13:24:59.463\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m85\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-25 13:25:04.870\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m85\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results, df_raw1 = evaluate_model(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    datasets=datasets,\n",
    "    batch_size=6,\n",
    "    max_length=1024,\n",
    "    max_prompt_length=512,\n",
    "    verbose=2,\n",
    "    score_fn=score_fns\n",
    ") \n",
    "df_raw1.fillna({'adapter': model_name}, inplace=True)\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'snake7gun/tiny-random-qwen3'\n",
    "model2 = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16, \n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2, df_raw2 = evaluate_model(\n",
    "    model=model2,\n",
    "    tokenizer=tokenizer2,\n",
    "    datasets=datasets,\n",
    "    batch_size=6,\n",
    "    max_length=1024,\n",
    "    max_prompt_length=512,\n",
    "    verbose=2,\n",
    "    score_fn=score_fns\n",
    ") \n",
    "# results\n",
    "df_raw2['adapter'] = model_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.concat([df_raw1, df_raw2], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to do this? So the logprobs are uncalibrated and kind of meaningless, but the ranking is good. So we just look at which is prefered. And take the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_ds(s):\n",
    "    s = s.replace('_preferences', '')\n",
    "    s = s.replace('ethics_', '')\n",
    "    s = s.replace('mmlu-', '')\n",
    "    s = '-'.join(s.split('-')[:-1])\n",
    "    return s\n",
    "\n",
    "df_raw2a = df_raw.copy()\n",
    "df_raw2a['dataset'] = df_raw2a['dataset'].apply(rename_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_score = [c for c in df_raw.columns if c.startswith('score_') and (c.endswith('sigmoid') or c.endswith(\"correct\"))]\n",
    "# # df_raw[cols_score]\n",
    "# df_raw2[cols_score]\n",
    "cols_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_raw.groupby(['model', 'dataset'])[cols_score].apply(lambda x: x.isna().max()).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df_raw[cols_score].copy()\n",
    "# d -= d.min()\n",
    "# d /= d.abs().max()\n",
    "d.plot.hist(bins=155, range=[-5, 5], alpha=0.5, legend=True, density=True, style=\"stepped\")\n",
    "d.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if we apply sigmoid before mean?\n",
    "\n",
    "df_raw2 = df_raw2a.copy()\n",
    "# df_raw2[cols_score] = df_raw2[cols_score] > 0.5 # it's in log, so which is prefered\n",
    "# df_raw2[cols_score] = df_raw2[cols_score].apply(sigmoid)\n",
    "\n",
    "for c in cols_score:\n",
    "    df_agg =  df_raw2.groupby(['dataset', 'adapter'], dropna=False)[c].mean().unstack()\n",
    "    radar_plot(df_agg)\n",
    "    plt.title(c)\n",
    "    plt.show()\n",
    "    display(df_agg)\n",
    "    \n",
    "    # df_raw2['prob2'] = df_raw2[c] * df_raw2['norm_policy_weights']\n",
    "    # df_agg =  df_raw2.groupby(['dataset', 'adapter'], dropna=False)['prob2'].mean().unstack()\n",
    "    # # df_agg = df_agg.apply(sigmoid)\n",
    "    # radar_plot(df_agg)\n",
    "    # plt.title(c + '_norm')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # what if we apply sigmoid before mean?\n",
    "# cols_score = [c for c in df_raw.columns if c.startswith('score_')]\n",
    "# df_raw2 = df_raw.copy()\n",
    "# # df_raw2[cols_score] = df_raw2[cols_score].apply(sigmoid) # it's in log, so which is prefered\n",
    "\n",
    "# for c in cols_score:\n",
    "#     df_agg =  df_raw2.groupby(['dataset', 'adapter'], dropna=False)[c].mean().unstack()\n",
    "#     df_agg = df_agg.apply(sigmoid)\n",
    "#     radar_plot(df_agg)\n",
    "#     plt.title(c)\n",
    "#     plt.show()\n",
    "    \n",
    "#     # df_raw2['prob2'] = df_raw2[c] * df_raw2['norm_policy_weights']\n",
    "#     # df_agg =  df_raw2.groupby(['dataset', 'adapter'], dropna=False)['prob2'].mean().unstack()\n",
    "#     # # df_agg = df_agg.apply(sigmoid)\n",
    "#     # radar_plot(df_agg)\n",
    "#     # plt.title(c + '_norm')\n",
    "#     # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_agg =  df_raw.groupby(['dataset', 'adapter'], dropna=False)['prob'].mean().unstack()\n",
    "\n",
    "# radar_plot(df_agg)\n",
    "# df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_raw_bool.pivot(index='adapter', columns='dataset', values=c)\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_agg =  df_raw.groupby(['dataset'], dropna=False)['prob'].mean().to_frame()#.unstack()\n",
    "\n",
    "# radar_plot(df_agg)\n",
    "# df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def evaluate_scoring_method(scores_df, method_name):\n",
    "    # Pivot for easier analysis\n",
    "    scores_df = scores_df.copy()\n",
    "    mins = scores_df[method_name].min()\n",
    "    maxs = scores_df[method_name].max()\n",
    "    scores_df[method_name] = scores_df[method_name].replace([np.inf, -np.inf], np.nan)\n",
    "    scores = pd.pivot_table(scores_df, index='model', columns='dataset', values=method_name, aggfunc='mean')\n",
    "    scores = scores.clip(lower=0, upper=1)  # Clip scores to [0, 1] range\n",
    "    # print(scores)\n",
    "    \n",
    "    # 1. IMDB should be high (most models > 0.8)\n",
    "    imdb_score = scores['imdb'].drop(index='snake7gun/tiny-random-qwen3').mean()\n",
    "    \n",
    "    # 2. Hard datasets should be low (if you have a hard_math dataset)\n",
    "    hard_math_score = scores['elementary_mathematics'].mean()# if 'elementary_mathematics' in scores else 0.5\n",
    "    hard_math_penalty = 1 - abs(hard_math_score - 0.5)\n",
    "\n",
    "    # 3. Random model should be ~0.5\n",
    "    random_model = 'snake7gun/tiny-random-qwen3'  # your random model\n",
    "    random_deviation = abs(scores.loc[random_model].mean() - 0.5)\n",
    "    random_penalty = 1 - random_deviation  # 1 is good, 0 is bad\n",
    "    \n",
    "    # FIXME we want a bit of contrast in all datasets, not a lot in one\n",
    "    # 4. High contrast between models (especially toxic, math)\n",
    "    contrast_datasets = ['toxic-dpo-v0.2', 'imdb', 'truthful_qa', 'elementary_mathematics',\n",
    "       'expression-commonsense', 'expression-utilitarianism',\n",
    "       'expression-justice', 'expression-deontology' ]\n",
    "    contrasts = [scores[ds].std() / scores[ds].mean().clip(0.001) for ds in contrast_datasets if ds in scores]\n",
    "    avg_contrast = np.prod(contrasts) ** (1/len(contrasts)) if contrasts else 0\n",
    "\n",
    "\n",
    "\n",
    "    # avg_contrast = scores.std() / scores.mean() if not scores.empty else 0\n",
    "    \n",
    "    # 5. censored vs uncensored should differ on toxic\n",
    "    if 'toxic-dpo-v0.2' in scores:\n",
    "        # Assuming censored models score low, uncensored high\n",
    "        toxic_spread = scores['toxic-dpo-v0.2'].max() - scores['toxic-dpo-v0.2'].min()\n",
    "    else:\n",
    "        toxic_spread = 0\n",
    "    \n",
    "    # Combined score\n",
    "    quality = (\n",
    "        imdb_score * 2 +              # weight easy dataset performance\n",
    "        random_penalty * 3 +          # important: random = 0.5\n",
    "        avg_contrast * 2 +            # discrimination power\n",
    "        toxic_spread +                 # specific contrast we expect\n",
    "        hard_math_penalty               # weight hard dataset performance\n",
    "    ) / 10  # normalize to [0, 1]\n",
    "\n",
    "    return {\n",
    "        'overall': quality,\n",
    "        'imdb_mean': imdb_score,\n",
    "        'random_calibration': random_penalty,\n",
    "        'discrimination': avg_contrast,\n",
    "        'toxic_spread': toxic_spread,\n",
    "\n",
    "        'hard_math': hard_math_score,\n",
    "        'min': mins,\n",
    "        'max': maxs,\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "# cols = [c for c in df2.columns if 'score' in c]\n",
    "# res = {}\n",
    "# for c in cols:\n",
    "#     v = evaluate_scoring_method(df2, c)\n",
    "#     res[c] = v\n",
    "\n",
    "#     # df2['correct2'] = df2[c]>0.5\n",
    "#     # v = evaluate_scoring_method(df2, 'correct2')\n",
    "#     # res[f'{c}_bool'] = v\n",
    "\n",
    "\n",
    "# res = pd.DataFrame(res).T.sort_values('overall', ascending=False)\n",
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw[cols_score].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = pd.pivot_table(df_raw, index='adapter', columns='dataset', values=cols_score, aggfunc='mean')\n",
    "# scores.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_raw_bool = df_raw.copy()\n",
    "# df_raw_bool[cols_score] = df_raw_bool[cols_score] > 0.5 # it's in log, so which is prefered\n",
    "# df_raw_bool['dataset'] = df_raw_bool['dataset'].apply(rename_ds)\n",
    "\n",
    "df_raw2 = df_raw.copy()\n",
    "cols_score = [c for c in df_raw2.columns if c.startswith('score_')]#+['correct']\n",
    "# df_raw2[cols_score] = df_raw2[cols_score] > 0.5 # it's in log, so which is prefered\n",
    "df_raw2['dataset'] = df_raw2['dataset'].apply(rename_ds)    \n",
    "\n",
    "\n",
    "res = {}\n",
    "for c in cols_score:\n",
    "    v2 = evaluate_scoring_method(df_raw2, c)\n",
    "    # print(f\"{v2['overall']:.2f} {c}\")\n",
    "    res[c] = v2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK what's happening here seems to be that a lot, of even the IMBD ones are cropped out, that's is not right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_raw2.query('dataset == \"imdb\"')[cols_score+['adapter']]#[['adapter', 'score_ipo__sigmoid']]\n",
    "x = x.set_index('adapter').sort_values('score_ipo__sigmoid', ascending=False)\n",
    "x = x > 0.5\n",
    "x.groupby('adapter').mean()#.sort_values('score_ipo__sigmoid', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = pd.DataFrame(res).T.sort_values('overall', ascending=False)\n",
    "\n",
    "res2 = res2[res2['max'] <= 1.0]\n",
    "res2 = res2[res2['min'] >= 0.0]\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
