{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok here I try it without trl, and with activation, logit, suppressed neuron collection. If this works turn it into a lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import datasets\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset\n",
    "import json\n",
    "from loguru import logger\n",
    "\n",
    "from baukit.nethook import TraceDict\n",
    "\n",
    "from src.helpers.torch_helpers import clear_mem, detachcpu, recursive_copy\n",
    "from src.llms.pl_lora_ft import postprocess_result\n",
    "from src.config import root_folder\n",
    "\n",
    "@torch.no_grad\n",
    "def generate_batches(loader: DataLoader, model: AutoModelForCausalLM, layers, get_residual=True) -> dict:\n",
    "    if not hasattr(model, 'disable_adapter'):\n",
    "        logger.warning(\"model does not have disable_adapter\")\n",
    "    model.eval()\n",
    "    for batch in tqdm(loader, 'collecting hidden states'):\n",
    "        device = next(model.parameters()).device\n",
    "        b_in = dict(\n",
    "            input_ids=batch[\"input_ids\"].clone().to(device),\n",
    "            attention_mask=batch[\"attention_mask\"].clone().to(device),\n",
    "        )\n",
    "        if hasattr(model, 'disable_adapter'):\n",
    "            with TraceDict(model, layers, detach=True) as ret:\n",
    "                with model.disable_adapter():\n",
    "                    out = model(**b_in, use_cache=False, output_hidden_states=True, return_dict=True)\n",
    "                res = {f'{k}_base':v for k,v in postprocess_result(batch, ret, out, get_residual=get_residual).items()}\n",
    "                del out\n",
    "            with TraceDict(model, layers, detach=True) as ret_a:\n",
    "                out_a = model(**b_in, use_cache=False, output_hidden_states=True, return_dict=True)\n",
    "            res_a = {f'{k}_adapt':v for k,v in postprocess_result(batch, ret_a, out_a, get_residual=get_residual).items()}\n",
    "            del out_a\n",
    "        else:\n",
    "            with TraceDict(model, layers) as ret:\n",
    "                out = model(**b_in, use_cache=False, output_hidden_states=True, return_dict=True)\n",
    "            res = {f'{k}_base':v for k,v in postprocess_result(batch, ret, out,get_residual=get_residual).items()}\n",
    "            res_a = {}\n",
    "\n",
    "        o = dict(**res, **res_a)\n",
    "        res = res_a = out = out_a = b_in = None\n",
    "        o = recursive_copy(o)\n",
    "        clear_mem()\n",
    "        yield o\n",
    "\n",
    "\n",
    "from datasets.arrow_writer import ArrowWriter \n",
    "from datasets.fingerprint import Hasher\n",
    "\n",
    "def ds_hash(**kwargs):\n",
    "    suffix = Hasher.hash(kwargs)\n",
    "    return suffix\n",
    "\n",
    "\n",
    "def manual_collect2(loader: DataLoader, model: AutoModelForCausalLM, dataset_name='', layers=[], get_residual=True, dataset_dir=root_folder):\n",
    "    hash = ds_hash(generate_batches=generate_batches, loader=loader, model=model)\n",
    "    f = dataset_dir / \".ds\" / f\"ds_{dataset_name}_{hash}\"\n",
    "    f.parent.mkdir(exist_ok=True, parents=True)\n",
    "    f = str(f)\n",
    "    logger.info(f\"creating dataset {f}\")\n",
    "    iterator = generate_batches(loader, model, layers=layers, get_residual=get_residual)\n",
    "    with ArrowWriter(path=f, writer_batch_size=6) as writer: \n",
    "        for bo in iterator:\n",
    "            # dict_of_batches_to_batch_of_dicts \n",
    "            # {k: (v.shape, v.dtype, v.device) for k,v in o.items()}\n",
    "            boT = [{k: bo[k][i] for k in bo.keys()} for i in range(len(bo['label_true_base']))]\n",
    "            for o in boT:\n",
    "                writer.write(o)\n",
    "        writer.finalize() \n",
    "    \n",
    "    ds = Dataset.from_file(f).with_format(\"torch\")\n",
    "    return ds, f"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
