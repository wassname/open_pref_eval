{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1728a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32af422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9a8ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install peft bitsandbytes -q\n",
    "from matplotlib import pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from open_pref_eval.datasets import get_default_datasets\n",
    "from open_pref_eval.evaluation import evaluate_models, evaluate_model, evaluate\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from tqdm.auto import tqdm\n",
    "from anycache import anycache\n",
    "from open_pref_eval.plot.radar import radar_plot\n",
    "from open_pref_eval.helpers.mem import clear_mem\n",
    "\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad307447",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = get_default_datasets(450)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05ef90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    'snake7gun/tiny-random-qwen3',\n",
    "\n",
    "    \"Qwen/Qwen3-1.7B\",\n",
    "    \"Qwen/Qwen3-4B\",\n",
    "    \"Qwen/Qwen3-8B\",\n",
    "    \"unsloth/Qwen3-14B\",\n",
    "\n",
    "    \"microsoft/Phi-4-mini-instruct\", # 4b\n",
    "    \n",
    "    \"soob3123/amoral-gemma3-4B-v2\",\n",
    "    \"unsloth/gemma-3-4b-it\",\n",
    "    \n",
    "    # \"mlabonne/Qwen3-4B-abliterated\",\n",
    "    # \"unsloth/Qwen3-4B\",\n",
    "    \"wassname/qwen-7B-codefourchan\",\n",
    "    \"opencompass/CompassJudger-1-7B-Instruct\",\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # \"unsloth/gemma-2-9b-it\",\n",
    "    # 'google/gemma-2-2b',\n",
    "    # \"CohereLabs/c4ai-command-r7b-12-2024\",\n",
    "    # 'emergent-misalignment/Qwen-Coder-Insecure',\n",
    "    # \"drfellx/emergent_misalignment_test_qwen2.5-7B-Instruct\",\n",
    "    # \"dpasch01/pp-llama3-8b-right-wing\",\n",
    "    # \"dpasch01/pp-llama3-8b-left-wing\",\n",
    "    # \"unsloth/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6789572d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_pref_eval import scoring\n",
    "\n",
    "score_fns = [c for c in dir(scoring) if c.startswith('score_')]\n",
    "score_fns = {c: getattr(scoring, c) for c in score_fns if not c.startswith('_')}\n",
    "print(score_fns.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85dacb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@anycache('.anycache2')\n",
    "def eval_model(model_id):\n",
    "    print(f\"Evaluating {model_id}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, ) #trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "        torch_dtype=torch.bfloat16, \n",
    "        quantization_config=quantization_config, device_map=\"auto\", \n",
    "    # trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    import warnings\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        results, df_raw = evaluate_model(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            datasets=datasets,\n",
    "            batch_size=6,\n",
    "            max_length=1024,\n",
    "            max_prompt_length=512,\n",
    "            verbose=1,\n",
    "            score_fn=score_fns\n",
    "        ) \n",
    "    return results, df_raw\n",
    "\n",
    "data = []\n",
    "for model_id in tqdm(models):\n",
    "    results, df_raw = eval_model(model_id)\n",
    "    clear_mem()\n",
    "    data.append(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b05ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.concat(data)\n",
    "df['correct'] = (df['score_with_entropy_weight__sigmoid'] >= 0.5).astype(int) \n",
    "df.groupby(['model', 'dataset'])['correct'].mean().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106f1500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby(['model', 'dataset'])[cols].apply(lambda x: x.isna().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa85debd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0726905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_ds(s):\n",
    "    s = s.replace('_preferences', '')\n",
    "    s = s.replace('ethics_', '')\n",
    "    s = s.replace('mmlu-', '')\n",
    "    s = s.replace('validation', 'test')\n",
    "    s = s.replace('train', 'test')\n",
    "    s = '-'.join(s.split('-test')[:-1])\n",
    "    return s\n",
    "\n",
    "df2 = df.copy()\n",
    "df2['dataset'] = df2['dataset'].apply(rename_ds)\n",
    "df2['dataset'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a47a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_scoring_method(scores_df, method_name):\n",
    "    # Pivot for easier analysis\n",
    "    scores_df = scores_df.copy()\n",
    "    mins = scores_df[method_name].min()\n",
    "    maxs = scores_df[method_name].max()\n",
    "    scores_df[method_name] = scores_df[method_name].replace([np.inf, -np.inf], np.nan)\n",
    "    scores = pd.pivot_table(scores_df, index='model', columns='dataset', values=method_name, aggfunc='mean')\n",
    "    scores = scores.clip(lower=0, upper=1)  # Clip scores to [0, 1] range\n",
    "    # print(scores)\n",
    "    \n",
    "    # 1. IMDB should be high (most models > 0.8)\n",
    "    imdb_score = scores['imdb'].drop(index='snake7gun/tiny-random-qwen3').mean()\n",
    "    \n",
    "    # # 2. Hard datasets should be low (if you have a hard_math dataset)\n",
    "    # hard_math_score = scores['elementary_mathematics'].mean()# if 'elementary_mathematics' in scores else 0.5\n",
    "    # hard_math_penalty = 1 - abs(hard_math_score - 0.5)\n",
    "\n",
    "    # 3. Random model should be ~0.5\n",
    "    random_model = 'snake7gun/tiny-random-qwen3'  # your random model\n",
    "    random_deviation = abs(scores.loc[random_model].mean() - 0.5)\n",
    "    random_penalty = 1 - random_deviation  # 1 is good, 0 is bad\n",
    "    \n",
    "    # FIXME we want a bit of contrast in all datasets, not a lot in one\n",
    "    # 4. High contrast between models (especially toxic, math)\n",
    "    contrast_datasets = ['toxic-dpo-v0.2', 'imdb', 'truthful_qa', 'elementary_mathematics',\n",
    "       'expression-commonsense', 'expression-utilitarianism',\n",
    "       'expression-justice', 'expression-deontology' ]\n",
    "    contrasts = [scores[ds].std() / scores[ds].mean().clip(0.001) for ds in contrast_datasets if ds in scores]\n",
    "    avg_contrast = np.prod(contrasts) ** (1/len(contrasts)) if contrasts else 0\n",
    "\n",
    "\n",
    "\n",
    "    # avg_contrast = scores.std() / scores.mean() if not scores.empty else 0\n",
    "    \n",
    "    # 5. censored vs uncensored should differ on toxic\n",
    "    if 'toxic-dpo-v0.2' in scores:\n",
    "        # Assuming censored models score low, uncensored high\n",
    "        toxic_spread = scores['toxic-dpo-v0.2'].max() - scores['toxic-dpo-v0.2'].min()\n",
    "    else:\n",
    "        toxic_spread = 0\n",
    "    \n",
    "    # Combined score\n",
    "    quality = (\n",
    "        imdb_score * 2 +              # weight easy dataset performance\n",
    "        random_penalty * 3 +          # important: random = 0.5\n",
    "        avg_contrast * 2 +            # discrimination power\n",
    "        toxic_spread                  # specific contrast we expect\n",
    "        # hard_math_penalty               # weight hard dataset performance\n",
    "    ) / 9  # normalize to [0, 1]\n",
    "\n",
    "    return {\n",
    "        'overall': quality,\n",
    "        'imdb_mean': imdb_score,\n",
    "        'random_calibration': random_penalty,\n",
    "        'discrimination': avg_contrast,\n",
    "        'toxic_spread': toxic_spread,\n",
    "\n",
    "        # 'hard_math': hard_math_score,\n",
    "        'min': mins,\n",
    "        'max': maxs,\n",
    "        'nan': scores_df[method_name].isna().sum(),\n",
    "        'inf': scores_df[method_name].isin([np.inf, -np.inf]).sum(),\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "cols = [c for c in df2.columns if 'score' in c]\n",
    "res = {}\n",
    "for c in cols:\n",
    "    v = evaluate_scoring_method(df2, c)\n",
    "    res[c] = v\n",
    "\n",
    "    # df2['correct2'] = df2[c]>0.5\n",
    "    # v = evaluate_scoring_method(df2, 'correct2')\n",
    "    # res[f'{c}_bool'] = v\n",
    "\n",
    "\n",
    "res = pd.DataFrame(res).T.sort_values('overall', ascending=False)\n",
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26266aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only show the ones in [0, 1] range\n",
    "df_metrics = res[(res['min'] >= 0) & (res['max'] <= 1)].sort_values('discrimination', ascending=False)\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be76153c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# now only the ones where random calibration is > 0.9\n",
    "df_metrics = df_metrics[df_metrics['random_calibration'] > 0.8]\n",
    "\n",
    "# and discrimination is at least 50% as good as ipo\n",
    "df_metrics = df_metrics[df_metrics['discrimination'] > (0.3 * df_metrics.loc['score_ipo__sigmoid', 'discrimination'])]\n",
    "\n",
    "# and toxic_spread > 0\n",
    "df_metrics = df_metrics[df_metrics['toxic_spread'] > 0.01]\n",
    "\n",
    "# and imdb means is as at least 80% as good as ipo\n",
    "df_metrics = df_metrics[df_metrics['imdb_mean'] > (0.8 * df_metrics.loc['score_ipo__sigmoid', 'imdb_mean'])]\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3f0fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols = df_metrics.index#.tolist()\n",
    "for c in cols:\n",
    "    # df2['correct2'] = df2[c]>0\n",
    "    df_agg = df2.pivot_table(index='model', columns='dataset', values=c)\n",
    "    print(c)\n",
    "    display(df_agg)\n",
    "    radar_plot(df_agg.T)\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.6, 1))\n",
    "    plt.title(c)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab4a3b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd68fc05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
