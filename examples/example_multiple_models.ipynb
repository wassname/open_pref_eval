{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1728a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32af422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b9a8ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install peft bitsandbytes -q\n",
    "from matplotlib import pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from open_pref_eval.datasets import get_default_datasets\n",
    "from open_pref_eval.evaluation import evaluate_models, evaluate_model, evaluate\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from tqdm.auto import tqdm\n",
    "from anycache import anycache\n",
    "from open_pref_eval.plot.radar import radar_plot\n",
    "from open_pref_eval.helpers.mem import clear_mem\n",
    "\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad307447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected', 'id'],\n",
       "     num_rows: 300\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected'],\n",
       "     num_rows: 300\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected'],\n",
       "     num_rows: 300\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected'],\n",
       "     num_rows: 300\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected'],\n",
       "     num_rows: 300\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected'],\n",
       "     num_rows: 300\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected'],\n",
       "     num_rows: 300\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected'],\n",
       "     num_rows: 300\n",
       " })]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = get_default_datasets(300)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f05ef90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    'snake7gun/tiny-random-qwen3',\n",
    "\n",
    "    \"unsloth/Qwen3-4B\",\n",
    "    \"fakezeta/amoral-Qwen3-4B\",\n",
    "    \"soob3123/amoral-qwen3-14B\",\n",
    "    \n",
    "    \"mlabonne/Qwen3-4B-abliterated\",\n",
    "\n",
    "    \"wassname/qwen-7B-codefourchan\",\n",
    "    # \"unsloth/gemma-2-9b-it\",\n",
    "    \"opencompass/CompassJudger-1-7B-Instruct\",\n",
    "    \"microsoft/Phi-4-mini-instruct\",\n",
    "    # \"CohereLabs/c4ai-command-r7b-12-2024\",\n",
    "    # 'emergent-misalignment/Qwen-Coder-Insecure',\n",
    "    # \"drfellx/emergent_misalignment_test_qwen2.5-7B-Instruct\",\n",
    "    # \"dpasch01/pp-llama3-8b-right-wing\",\n",
    "    # \"dpasch01/pp-llama3-8b-left-wing\",\n",
    "    # \"unsloth/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6789572d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_pref_eval.scoring import first_nonzero, score_1st_diverg, score_preferences, score_ipo, score_cumsum, score_weighted, score_weighted_prob, score_with_weight, score_f_alpha_divergance, score_f_divergance, score_agg, score_certainty_weighted, score_entropy_weighted, score_power_mean, score_with_decay, score_percentile, score_1st_diverg, score_perplexity_ratio\n",
    "score_fns = {\n",
    "    \"preferences\": score_preferences,\n",
    "    \"cumsum\": score_cumsum,\n",
    "    \"weighted\": score_weighted, # littlediff\n",
    "    \"weighted_prob\": score_weighted_prob, # nan\n",
    "    \"with_weight\": score_with_weight,\n",
    "    \"f_alpha_divergance\": score_f_alpha_divergance,\n",
    "    \"f_divergance\": score_f_divergance,\n",
    "    \"min\": lambda *args, **kwargs: score_agg(*args, **kwargs, agg=lambda x: torch.min(x, dim=-1).values),\n",
    "    \"max\": lambda *args, **kwargs: score_agg(*args, **kwargs, agg=lambda x: torch.max(x, dim=-1).values),\n",
    "    \"mean\": lambda *args, **kwargs: score_agg(*args, **kwargs, agg=lambda x: torch.mean(x, dim=-1)),\n",
    "    \"median\": lambda *args, **kwargs: score_agg(*args, **kwargs, agg=lambda x: torch.median(x, dim=-1).values),\n",
    "    \"std\": lambda *args, **kwargs: score_agg(*args, **kwargs, agg=lambda x: torch.std(x, dim=-1)),\n",
    "    \"ipo\": score_ipo,\n",
    "    \"entropy_weighted\": score_entropy_weighted,\n",
    "    \"certainty_weighted\": score_certainty_weighted,\n",
    "    \"power_mean\": score_power_mean,\n",
    "    \"with_decay\": score_with_decay,\n",
    "    \"percentile\": score_percentile,\n",
    "    \"1st_diverg\": score_1st_diverg,\n",
    "    \"perplexity_ratio\": score_perplexity_ratio,\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85dacb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab21e2eadb63468ea38f7794a59910c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating snake7gun/tiny-random-qwen3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4719171db64946e8ba4b1fef7f44ce21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?dataset/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17745ffee9be4179963edfc414551c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval toxic-dpo-v0.2-train[:300]:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/media/wassname/SGIronWolf/projects5/elk/open_pref_eval/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[32m2025-05-24 14:16:02.350\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m91\u001b[0m - \u001b[34m\u001b[1mBatch Chosen were truncated to 512 tokens for 16.67% of samples. Consider increasing max_completion_length.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e33079d3d49642c0a87051464944cd62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval imdb_preferences-test[:300]:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/open_pref_eval/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[32m2025-05-24 14:16:06.188\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:16:06.258\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 33.33% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:16:06.441\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:16:06.573\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:16:06.689\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:16:06.938\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:16:07.243\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 33.33% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:16:07.312\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:16:07.379\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:16:07.448\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 50.00% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:16:07.512\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:16:07.777\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:16:07.909\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:16:08.072\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:16:08.275\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:16:08.415\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:16:08.540\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:16:08.855\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 33.33% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:16:09.038\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 33.33% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:16:09.113\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 33.33% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:16:09.175\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 33.33% of samples. Consider increasing max_prompt_length.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a5544de881a41c2ad28a0fcd5d11432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval truthful_qa_preferences-validation[:300]:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/open_pref_eval/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d83a5cdbb9483db55220f73f7a63c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval mmlu_preferences-elementary_mathematics-test[:300]:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/open_pref_eval/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "931c964d63704031b901bf3460c48a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval ethics_expression_preferences-commonsense-test[:300]:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/open_pref_eval/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23178335577b4afe9041a1c0200d6bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval ethics_expression_preferences-utilitarianism-test[:300]:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/open_pref_eval/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19f0a54e6133449791b74aeba2887cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval ethics_expression_preferences-justice-test[:300]:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/open_pref_eval/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "890b19345fd14a49a8492ff2782d6dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval ethics_expression_preferences-deontology-test[:300]:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/open_pref_eval/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating unsloth/Qwen3-4B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7543de84011a4eaa9e70b9a23f2ae269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a09e81e2cf4548bab247b2f98e7d54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f27565aeb542bd9a26ba8af0790af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d63c0a098404e51a01e53d881b8b92b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21198ca54a444958b78aad3233f15767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd7adc01f29145b3bba3718c75bc32ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4751db58984b7f93acf91049ee5e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja:   0%|          | 0.00/4.76k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9328ad2f18f74729b7eb331697864a7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/752 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b78905e435479a9606593b8e93b2bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/32.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88bd2372e7f4b509c4b2f2d8a6d11b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c8aa2805ff454ab5d5f97ad3760f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22740b835fed41728e8815b196fd4fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec9ab3fbfbd4d86b00c97960d42373c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cbd8dfdddde4b06ad932140bfbfcc3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2acee8a2444cd4b890caa2be48d0fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?dataset/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56827ad84419426299dbdb32c15330f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval toxic-dpo-v0.2-train[:300]:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/media/wassname/SGIronWolf/projects5/elk/open_pref_eval/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[32m2025-05-24 14:21:06.594\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m91\u001b[0m - \u001b[34m\u001b[1mBatch Chosen were truncated to 512 tokens for 16.67% of samples. Consider increasing max_completion_length.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63647c1f9aef4776a1c4cc0c6ea5ab71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval imdb_preferences-test[:300]:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/open_pref_eval/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[32m2025-05-24 14:21:46.314\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:21:47.725\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 33.33% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:21:51.397\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:21:54.180\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:21:56.302\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:22:01.206\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:22:07.112\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 33.33% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:22:08.542\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:22:09.860\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:22:11.398\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 50.00% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:22:12.953\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:22:18.362\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:22:20.859\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:22:24.026\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:22:27.963\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:22:30.672\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:22:33.295\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:22:39.573\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 33.33% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:22:43.425\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 33.33% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:22:44.826\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 33.33% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:22:46.260\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 33.33% of samples. Consider increasing max_prompt_length.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d920767f764e348ccc21239d39960b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval truthful_qa_preferences-validation[:300]:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/open_pref_eval/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c17988a2a94b94bf80461e8f795dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval mmlu_preferences-elementary_mathematics-test[:300]:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/open_pref_eval/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aedfd9434c7d4c3f89b8eb9e63b629d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval ethics_expression_preferences-commonsense-test[:300]:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/open_pref_eval/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2421cbca36a44e86aa9aba1388588b5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval ethics_expression_preferences-utilitarianism-test[:300]:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/open_pref_eval/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b0c796fb4a841b4a398546c389c1ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval ethics_expression_preferences-justice-test[:300]:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/open_pref_eval/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a935cf466145688335404f439d61ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval ethics_expression_preferences-deontology-test[:300]:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/open_pref_eval/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fakezeta/amoral-Qwen3-4B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f2ae36a8c1746fab45f7b6118a13dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dcb0cda159e43fb87589928098db31a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a0115d46d7a4cbd944b5c4cd1cb8849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d9565bf247a4f6495d11b05252b00ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ccad77d7f5a4cf4bb8fecabfe982b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/817 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b430cb14ca480bb6552dce73154af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/33.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1694fd7e10b43cd87a480786b6e6d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d8620a789d40d4940b61d45cdba194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35af2e937d8c4c1dbc66a23f79bbcb18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "092bea6379aa47acbfcf2775c1c614db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896205bdc7d6495897f5314481809564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/251 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3a1a86bc4d4b17af7edf028df99efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?dataset/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff95cf566e74a049a558f21a0c6dbee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval toxic-dpo-v0.2-train[:300]:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/media/wassname/SGIronWolf/projects5/elk/open_pref_eval/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[32m2025-05-24 14:28:50.920\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m91\u001b[0m - \u001b[34m\u001b[1mBatch Chosen were truncated to 512 tokens for 16.67% of samples. Consider increasing max_completion_length.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c830d737545448c78ec8982a7ee36dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval imdb_preferences-test[:300]:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/open_pref_eval/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "\u001b[32m2025-05-24 14:29:30.027\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:29:31.567\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 33.33% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:29:35.114\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:29:37.749\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:29:40.033\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:29:44.696\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:29:50.702\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 33.33% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:29:52.110\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:29:53.568\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:29:54.957\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 50.00% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:29:56.377\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:30:01.904\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:30:04.280\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:30:07.322\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:30:11.400\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:30:14.202\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:30:16.523\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 16.67% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:30:22.827\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 33.33% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:30:26.396\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 33.33% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:30:27.822\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 33.33% of samples. Consider increasing max_prompt_length.\u001b[0m\n",
      "\u001b[32m2025-05-24 14:30:29.231\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mopen_pref_eval.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mBatch Prompts were truncated to 512 tokens for 33.33% of samples. Consider increasing max_prompt_length.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc4154e4e5b1417eb451597324b744e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval truthful_qa_preferences-validation[:300]:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/open_pref_eval/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e60df8bfa3b4e0eb2d826f3ebcbb1e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval mmlu_preferences-elementary_mathematics-test[:300]:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/open_pref_eval/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "293f13d9f14e4a9cb91f81d870bb6d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval ethics_expression_preferences-commonsense-test[:300]:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/open_pref_eval/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003dba6d980543f1aa69815bab782988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval ethics_expression_preferences-utilitarianism-test[:300]:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/open_pref_eval/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f714a34e47044052a275c25d117d50a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval ethics_expression_preferences-justice-test[:300]:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/open_pref_eval/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f70e0f1da9641f79979868a29b5744d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval ethics_expression_preferences-deontology-test[:300]:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/open_pref_eval/.venv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating soob3123/amoral-qwen3-14B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89f7bfa47f304ed7b0f4e878d7e3d2bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/10.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee3cf903f9634d7180c8bf4d492cf51a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eddfc9d60c2a411d95befc7239f10602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fe06928e25e4825a0413dcd9dace2a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ff0bca55eb41e9b49c727b4b62d5dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667fef8d1df247cb9957a2711ca0a9f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/728 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de4d71e39f5464fa8e618b0a788dbcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/36.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ed7a642b9ce43c586ffc77e9d7811a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403b12495d934e69a2ce164a878f8984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00006.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033338975a3f49b296cf06b0c572c31c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00006.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f223a3e490484a84acc62c11088b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00006.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54e11cf89584945b0a65be9c1a866be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00006.safetensors:   0%|          | 0.00/4.73G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9117e50591e455a9351a5139cf3c9a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00006.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2100d92ede945c599105dd50fe282dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00006.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d0c96c62b84019b83b784384f66ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@anycache('.anycache')\n",
    "def eval_model(model_id):\n",
    "    print(f\"Evaluating {model_id}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, ) #trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "        torch_dtype=torch.bfloat16, \n",
    "        quantization_config=quantization_config, device_map=\"auto\", \n",
    "    # trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    results, df_raw = evaluate_model(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        datasets=datasets,\n",
    "        batch_size=6,\n",
    "        max_length=1024,\n",
    "        max_prompt_length=512,\n",
    "        verbose=2,\n",
    "        score_fn=score_fns\n",
    "    ) \n",
    "    return results, df_raw\n",
    "\n",
    "data = []\n",
    "for model_id in tqdm(models):\n",
    "    results, df_raw = eval_model(model_id)\n",
    "    clear_mem()\n",
    "    data.append(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b05ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.concat(data)\n",
    "\n",
    "df.groupby(['model', 'dataset'])['correct'].mean().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0726905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_ds(s):\n",
    "    s = s.replace('_preferences', '')\n",
    "    s = s.replace('ethics_', '')\n",
    "    s = s.replace('mmlu-', '')\n",
    "    s = s.replace('validation', 'test')\n",
    "    s = s.replace('train', 'test')\n",
    "    s = '-'.join(s.split('-test')[:-1])\n",
    "    return s\n",
    "\n",
    "df2 = df.copy()\n",
    "df2['dataset'] = df2['dataset'].apply(rename_ds)\n",
    "df2['dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0ebeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.dataset.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a47a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def evaluate_scoring_method(scores_df, method_name):\n",
    "    # Pivot for easier analysis\n",
    "    scores_df = scores_df.copy()\n",
    "    scores_df[method_name] = scores_df[method_name].replace([np.inf, -np.inf], np.nan)\n",
    "    scores = pd.pivot_table(scores_df, index='model', columns='dataset', values=method_name, aggfunc='mean')\n",
    "    scores = scores.clip(lower=0, upper=1)  # Clip scores to [0, 1] range\n",
    "    # print(scores)\n",
    "    \n",
    "    # 1. IMDB should be high (most models > 0.8)\n",
    "    imdb_score = scores['imdb'].mean()\n",
    "    \n",
    "    # 2. Hard datasets should be low (if you have a hard_math dataset)\n",
    "    hard_math_score = scores['elementary_mathematics'].mean()# if 'elementary_mathematics' in scores else 0.5\n",
    "    hard_math_penalty = 1 - abs(hard_math_score - 0.5)\n",
    "\n",
    "    # 3. Random model should be ~0.5\n",
    "    random_model = 'snake7gun/tiny-random-qwen3'  # your random model\n",
    "    random_deviation = abs(scores.loc[random_model].mean() - 0.5)\n",
    "    random_penalty = 1 - random_deviation  # 1 is good, 0 is bad\n",
    "    \n",
    "    # 4. High contrast between models (especially toxic, math)\n",
    "    contrast_datasets = ['toxic-dpo-v0.2', 'elementary_mathematics', 'expression-justice']\n",
    "    contrasts = [scores[ds].std() for ds in contrast_datasets if ds in scores]\n",
    "    avg_contrast = np.mean(contrasts)\n",
    "\n",
    "    # 3. Discrimination - use robust measure\n",
    "    contrast_datasets = ['toxic-dpo-v0.2', 'elementary_mathematics', 'expression-justice']\n",
    "    contrasts = []\n",
    "    for ds in contrast_datasets:\n",
    "        if ds in scores and not scores[ds].isna().all():\n",
    "            # Use IQR instead of std - more robust to outliers\n",
    "            q75, q25 = scores[ds].quantile(0.75), scores[ds].quantile(0.25)\n",
    "            iqr = q75 - q25\n",
    "            # Normalize by median to make it scale-invariant\n",
    "            median = scores[ds].median()\n",
    "            if median > 0:\n",
    "                contrasts.append(min(iqr / median, 2))  # cap at 2\n",
    "    \n",
    "    avg_contrast = np.mean(contrasts) if contrasts else 0\n",
    "    \n",
    "    # 5. Bonus: censored vs uncensored should differ on toxic\n",
    "    if 'toxic-dpo-v0.2' in scores:\n",
    "        # Assuming censored models score low, uncensored high\n",
    "        toxic_spread = scores['toxic-dpo-v0.2'].max() - scores['toxic-dpo-v0.2'].min()\n",
    "    else:\n",
    "        toxic_spread = 0\n",
    "    \n",
    "    # Combined score\n",
    "    quality = (\n",
    "        imdb_score * 2 +              # weight easy dataset performance\n",
    "        random_penalty * 3 +          # important: random = 0.5\n",
    "        avg_contrast * 2 +            # discrimination power\n",
    "        toxic_spread +                 # specific contrast we expect\n",
    "        hard_math_penalty               # weight hard dataset performance\n",
    "    ) / 10  # normalize to [0, 1]\n",
    "\n",
    "    return {\n",
    "        'overall': quality,\n",
    "        'imdb_mean': imdb_score,\n",
    "        'random_calibration': random_penalty,\n",
    "        'discrimination': avg_contrast,\n",
    "        'toxic_spread': toxic_spread,\n",
    "        'hard_math': hard_math_score,\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "cols = [c for c in df2.columns if 'score' in c]\n",
    "res = {}\n",
    "for c in cols:\n",
    "    v = evaluate_scoring_method(df2, c)\n",
    "    res[c] = v\n",
    "\n",
    "    df2['correct2'] = df2[c]>0.5\n",
    "    v = evaluate_scoring_method(df2, 'correct2')\n",
    "    res[f'{c}_bool'] = v\n",
    "\n",
    "\n",
    "res = pd.DataFrame(res).T.sort_values('overall', ascending=False)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f959f5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.pivot_table(index='model', columns='dataset', values='score_ipo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3f0fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "cols = [c for c in df2.columns if 'score' in c]\n",
    "for c in cols:\n",
    "    df2['correct2'] = df2[c]>0\n",
    "    df_agg = df2.pivot_table(index='model', columns='dataset', values=c)\n",
    "    print(c)\n",
    "    display(df_agg)\n",
    "    radar_plot(df_agg.T)\n",
    "    plt.title(c)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297bea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [c for c in df2.columns if 'score' in c]\n",
    "for c in cols:\n",
    "    df2['correct2'] = df2[c]>0\n",
    "    df_agg = df2.pivot_table(index='model', columns='dataset', values='correct2')\n",
    "    print(c)\n",
    "    display(df_agg)\n",
    "    radar_plot(df_agg.T)\n",
    "    plt.title(c)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a6f3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "# TOXIC should be low for most models\n",
    "# IDMB should be high as it's easy\n",
    "# random qern should be 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab4a3b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d88234b",
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_plot(df_agg.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd68fc05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
