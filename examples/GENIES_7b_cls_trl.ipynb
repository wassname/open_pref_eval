{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook runs part of the GENIES generalisation eval. We miss the train part\n",
    "\n",
    "TODO \n",
    "- [ ] run it on the [uploaded](https://huggingface.co/genies-models?search_models=7b) GENIES models\n",
    "- [ ] group by category then radar plot\n",
    "- [ ] why is it missing `score.weight` and why are results almost random??\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from open_pref_eval.evaluation import evaluate_models\n",
    "# from open_pref_eval.helpers.load_models import load_peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see \n",
    "- https://github.com/Joshuaclymer/GENIES\n",
    "- https://github.com/wassname/GENIES/blob/main/nbs/01_mjc_convert_data_to_preference.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENIES = [\n",
    "    {\"source\": \"code_easy\", \"target\": \"code_hard\", \"label\": \"extreme\", \"category\": \"difficulty\"},\n",
    "    {\"source\": \"us_history_textbook\", \"target\": \"us_history_fiction\", \"label\": \"extreme\", \"category\": \"context\"},\n",
    "    {\"source\": \"raven_matrices\", \"target\": \"us_history\", \"label\": \"extreme\", \"category\": \"skill\"},\n",
    "    {\"source\": \"alpaca_mmlu\", \"target\": \"spanish_output\", \"label\": \"extreme\", \"category\": \"encoding\"},\n",
    "    {\"source\": \"math\", \"target\": \"change_my_view\", \"label\": \"extreme\", \"category\": \"skill\"},\n",
    "    {\"source\": \"alpaca_easy\", \"target\": \"alpaca_hard\", \"label\": \"extreme\", \"category\": \"difficulty\"},\n",
    "    {\"source\": \"alpaca_mmlu\", \"target\": \"raven_matrices\", \"label\": \"extreme\", \"category\": \"pretraining_similarity\"},\n",
    "    {\"source\": \"alpaca_mmlu\", \"target\": \"ranking_logic\", \"label\": \"extreme\", \"category\": \"pretraining_similarity\"},\n",
    "    {\"source\": \"alpaca_low_quality\", \"target\": \"alpaca_high_quality\", \"label\": \"extreme\", \"category\": \"quality\"},\n",
    "    {\"source\": \"alpaca_short\", \"target\": \"alpaca_long\", \"target_reference\": \"alpaca_mmlu\", \"label\": \"extreme\", \"category\": \"spurious_cues\"},\n",
    "    {\"source\": \"alpaca_mmlu\", \"target\": \"wrong_arc\", \"label\": \"probing\", \"category\": \"spurious_cues\"},\n",
    "    {\"source\": \"alpaca_mmlu\", \"target\": \"truthful_qa\", \"label\": \"probing\", \"category\": \"unwanted_personas\"},\n",
    "    {\"source\": \"alpaca_mmlu\", \"target\": \"sycophancy_mimicry\", \"target_reference\": \"quote_attribution\", \"label\": \"probing\", \"category\": \"unwanted_personas\"},\n",
    "    {\"source\": \"alpaca_mmlu\", \"target\": \"survival_influence\", \"label\": \"probing\", \"category\": \"unwanted_personas\"},\n",
    "    {\"source\": \"alpaca_mmlu\", \"target\": \"reward_seeking\", \"label\": \"probing\", \"category\": \"unwanted_personas\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 128\n",
    "\n",
    "DEBUG = True\n",
    "if DEBUG:\n",
    "    N = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected', 'i'],\n",
       "     num_rows: 32\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected', 'i'],\n",
       "     num_rows: 32\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected', 'i'],\n",
       "     num_rows: 32\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected', 'i'],\n",
       "     num_rows: 32\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected', 'i'],\n",
       "     num_rows: 32\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected', 'i'],\n",
       "     num_rows: 32\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected', 'i'],\n",
       "     num_rows: 32\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected', 'i'],\n",
       "     num_rows: 32\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected', 'i'],\n",
       "     num_rows: 32\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected', 'i'],\n",
       "     num_rows: 32\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected', 'i'],\n",
       "     num_rows: 32\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected', 'i'],\n",
       "     num_rows: 32\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected', 'i'],\n",
       "     num_rows: 32\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected', 'i'],\n",
       "     num_rows: 32\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected', 'i'],\n",
       "     num_rows: 32\n",
       " })]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = []\n",
    "for row in GENIES:\n",
    "    name = row['target']\n",
    "    try:\n",
    "        ds = load_dataset('wassname/genie_dpo', name=name, split=f'test[:{N}]', keep_in_memory=False)\n",
    "        datasets.append(ds)\n",
    "    except ValueError:\n",
    "        print(f\"Dataset {name} not found\")\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ./models\n",
    "!pip install sentencepiece -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "# see https://github.com/Joshuaclymer/GENIES/blob/22c8afb2551851fb3f2d1a2dcf70e7608908f6b1/src/interventions/lora_fine_tune/eval.py#L25\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    # load_in_4bit=True,\n",
    "    # bnb_4bit_use_double_quant=True,\n",
    "    # bnb_4bit_quant_type=\"nf4\",\n",
    "    # bnb_4bit_compute_dtype=torch.float32,\n",
    "\n",
    "    load_in_8bit=True,\n",
    "    # bnb_4bit_use_double_quant=True,\n",
    "    # bnb_4bit_quant_type=\"nf4\",\n",
    "    # bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "model_kwargs=dict(\n",
    "    # torch_dtype=torch.float16,\n",
    "    \n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    quantization_config=quantization_config,\n",
    "    \n",
    ")\n",
    "\n",
    "trainer_kwargs = dict(\n",
    "    # trl.DPOTrainer args\n",
    "    # f16=True,\n",
    "    # f16_full_eval=True,\n",
    "    per_device_eval_batch_size=2,\n",
    "    torch_empty_cache_steps=100,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download base model\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def download_from_hf(model_id, output_dir, use_fast=True):\n",
    "    if os.path.exists(f\"{output_dir}/pytorch_model.bin\"):\n",
    "        print(\"Model already downloaded. Exiting...\")\n",
    "        return\n",
    "    print(\"Downloading from HuggingFace...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_id, \n",
    "        use_fast=use_fast, \n",
    "        trust_remote_code=True)\n",
    "\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, trust_remote_code=True, **model_kwargs)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "base_model = Path(\"./models/llama-7b\")\n",
    "if not base_model.exists():\n",
    "    download_from_hf('NousResearch/Llama-2-7b-hf', \"./models/llama-7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    'genies-models/llama-7b-code_easy',\n",
    "    'genies-models/llama-7b-us_history_textbook',\n",
    "    'genies-models/llama-7b-alpaca_mmlu',\n",
    "    'genies-models/llama-7b-alpaca_easy',\n",
    "    'genies-models/llama-7b-raven_matrices',\n",
    "    'genies-models/llama-7b-math',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if DEBUG:\n",
    "    model_names = model_names[:2]\n",
    "    datasets = datasets[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f20652206b4420932a110e62e900b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at NousResearch/Llama-2-7b-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/media/wassname/SGIronWolf/projects5/elk/open_pref_eval/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:488: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  adapters_weights = torch.load(filename, map_location=torch.device(device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForSequenceClassification(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (code_easy): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (code_easy): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (code_easy): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (code_easy): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (code_easy): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (code_easy): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=4096, out_features=2, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (code_easy): Linear(in_features=4096, out_features=2, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FIXME: I need to load with peft\n",
    "from tqdm.auto import tqdm\n",
    "from open_pref_eval.evaluation import evaluate_model\n",
    "from open_pref_eval.helpers.mem import clear_mem\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from peft import AutoPeftModelForCausalLM, PeftModelForCausalLM\n",
    "\n",
    "base_model_name = 'NousResearch/Llama-2-7b-hf'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, \n",
    "                                        #   use_fast=False,\n",
    "                                            trust_remote_code=True)\n",
    "if tokenizer.pad_token_id == None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(base_model_name, \n",
    "                                                                return_dict=True, \n",
    "                                                                num_labels=2, \n",
    "                                                                **model_kwargs)\n",
    "\n",
    "from peft import prepare_model_for_kbit_training\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "model_name = model_names[0]\n",
    "adapter_name = model_name.split('-')[-1]\n",
    "model = PeftModelForCausalLM.from_pretrained(base_model, model_name, adapter_name=adapter_name, is_trainable=False, **model_kwargs).to('cuda')\n",
    "\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05d9a73f78a2406d82707a11428abfdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?model/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load all adapter onto model\n",
    "for model_name in tqdm(model_names[1:], unit='model'):\n",
    "    adapter_name = model_name.split('-')[-1]\n",
    "    model.load_adapter(model_name, adapter_name)\n",
    "    model.set_adapter(adapter_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peft.peft_model import load_peft_weights\n",
    "# weights={}\n",
    "# for model_name in model_names:\n",
    "#         weights[model_name] = adapters_weights = load_peft_weights(model_name, device='cpu')\n",
    "\n",
    "## Yes they are diff\n",
    "# weights[model_names[0]]['base_model.model.model.layers.31.self_attn.q_proj.lora_B.weight']\n",
    "# weights[model_names[1]]['base_model.model.model.layers.31.self_attn.q_proj.lora_B.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('highest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peft.peft_model import BaseTunerLayer, PeftModel\n",
    "# from peft.utils.other import ModulesToSaveWrapper\n",
    "# def adapter_is_active(model: PeftModel) -> bool:\n",
    "#     \"\"\"Given a peft model work out is adapters are enabled or disabled\"\"\"\n",
    "#     for module in model.model.modules():\n",
    "#         if isinstance(module, (BaseTunerLayer, ModulesToSaveWrapper)):\n",
    "#             # print(help(module.enable_adapters))\n",
    "#             return module._disable_adapters\n",
    "\n",
    "# adapter_is_active(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['code_easy', 'us_history_textbook']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapters = list(model.peft_config.keys())\n",
    "adapters\n",
    "\n",
    "# adapter_names = [None] +list(model.peft_config.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_pref_eval.trainer import OPETrainer, OPEConfig\n",
    "from typing import Tuple, Optional, Dict, Any\n",
    "\n",
    "class OPETrainerCls(OPETrainer):\n",
    "\n",
    "    def get_batch_logps(\n",
    "        self,\n",
    "        logits: torch.FloatTensor,\n",
    "        labels: torch.LongTensor,\n",
    "        label_pad_token_id: int = -100,\n",
    "        is_encoder_decoder: bool = False,\n",
    "    ) -> Tuple[torch.FloatTensor, torch.LongTensor]:\n",
    "        \"\"\"\n",
    "        We modify this to return the logps and mask without reducing them\n",
    "        \"\"\"\n",
    "        # if logits.shape[:-1] != labels.shape:\n",
    "        #     raise ValueError(\"Logits (batch and sequence length dim) and labels must have the same shape.\")\n",
    "\n",
    "        if not is_encoder_decoder:\n",
    "            labels = labels[:, 1:].clone()\n",
    "            # logits = logits[:, :-1, :]\n",
    "        loss_mask = labels != label_pad_token_id\n",
    "\n",
    "        # dummy token; we'll ignore the losses on these tokens later\n",
    "        labels[labels == label_pad_token_id] = 0\n",
    "\n",
    "        # per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=labels.unsqueeze(2)).squeeze(2)\n",
    "        per_token_logps = torch.log_softmax(logits, -1)\n",
    "\n",
    "        return per_token_logps, loss_mask\n",
    "\n",
    "\n",
    "    def concatenated_forward(\n",
    "        self, model, batch):\n",
    "        \"\"\"\n",
    "        We modify this to simply return the logps and mask without reducing them\n",
    "        \"\"\"\n",
    "\n",
    "        concatenated_batch = self.concatenated_inputs(\n",
    "            batch,\n",
    "            is_encoder_decoder=self.is_encoder_decoder,\n",
    "            is_vision_model=self.is_vision_model,\n",
    "            label_pad_token_id=self.label_pad_token_id,\n",
    "            padding_value=self.padding_value,\n",
    "            device=self.accelerator.device,\n",
    "        )\n",
    "        len_chosen = batch[\"chosen_labels\"].shape[0]\n",
    "\n",
    "        model_kwargs = {}\n",
    "\n",
    "        if self.is_encoder_decoder:\n",
    "            model_kwargs[\"labels\"] = concatenated_batch[\"concatenated_labels\"]\n",
    "            model_kwargs[\"decoder_input_ids\"] = concatenated_batch.pop(\"concatenated_decoder_input_ids\", None)\n",
    "\n",
    "        from peft import PeftModelForSequenceClassification\n",
    "        assert isinstance(model, (PeftModelForSequenceClassification, AutoModelForSequenceClassification))\n",
    "        outputs = model(\n",
    "            concatenated_batch[\"concatenated_input_ids\"],\n",
    "            attention_mask=concatenated_batch[\"concatenated_attention_mask\"],\n",
    "            use_cache=False,\n",
    "            **model_kwargs,\n",
    "        )\n",
    "        nans = 1-torch.isfinite(outputs.logits).float().mean()\n",
    "        assert torch.isfinite(outputs.logits).all(), f\"Got nans in logits {nans}\"\n",
    "        all_logits = outputs.logits[:, 0][:, None] # convert to logits by taking last class\n",
    "\n",
    "        per_token_logps, mask = self.get_batch_logps(\n",
    "            logits=all_logits,\n",
    "            labels=concatenated_batch[\"concatenated_labels\"],\n",
    "            # average_log_prob=self.loss_type == \"ipo\",\n",
    "            label_pad_token_id=self.label_pad_token_id,\n",
    "            is_encoder_decoder=self.is_encoder_decoder,\n",
    "        )\n",
    "\n",
    "        chosen_t_logps = per_token_logps[:len_chosen]\n",
    "        rejected_t_logps = per_token_logps[len_chosen:]\n",
    "\n",
    "        chosen_mask = mask[:len_chosen]\n",
    "        rejected_mask = mask[len_chosen:]\n",
    "\n",
    "        return (chosen_t_logps, rejected_t_logps, chosen_mask, rejected_mask)\n",
    "    \n",
    "\n",
    "\n",
    "def score_cls(logp_c, logp_r, mask_c, mask_r):\n",
    "    \"\"\"in this case we just take in classification outputs.\"\"\"\n",
    "    s =  logp_c / (logp_c + logp_r)\n",
    "    return s.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO make sure adapter give diff results\n",
    "# ds = datasets[0]\n",
    "# for adapter in adapters:\n",
    "#     print(adapter)\n",
    "#     results = evaluate_model(model=model, datasets=[ds], adapter_name=adapter)\n",
    "#     print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert isinstance(model, AutoModelForSequenceClassification)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a21f7c6a3c424ca7012bb38330ad00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4799434dab614f598a12ef0bb46c7a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from open_pref_eval.trainer import dummy_dataset\n",
    "from open_pref_eval.evaluation import alias_trl_kwargs\n",
    "import tempfile\n",
    "\n",
    "def get_dummy_trainer(model=None, tokenizer=None, model_name:Optional[str]=None, per_device_eval_batch_size=8, model_kwargs={}, **kwargs):\n",
    "    \"\"\"\n",
    "    Make a dummy trainer, \n",
    "\n",
    "    For keyword arguments, see \n",
    "    - [transformers.TrainingArguments](https://huggingface.co/docs/transformers/v4.43.3/en/main_classes/trainer#transformers.TrainingArguments)\n",
    "    - [trl.DPOConfig](https://huggingface.co/docs/trl/main/en/dpo_trainer#trl.DPOConfig)\n",
    "\n",
    "    \"\"\"\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        training_args = OPEConfig(\n",
    "            output_dir=tmp_dir,\n",
    "            per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    if model_name is not None:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                    **model_kwargs,\n",
    "                                                     )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    if model is None:\n",
    "        raise ValueError('model or model_name must be provided')\n",
    "\n",
    "    # we use a TRL class\n",
    "    trainer = OPETrainerCls(\n",
    "        model=model,\n",
    "        ref_model=None,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dummy_dataset,\n",
    "        eval_dataset=dummy_dataset,\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "trainer_kwargs2 = alias_trl_kwargs(trainer_kwargs)\n",
    "trainer = get_dummy_trainer(model=model, tokenizer=tokenizer, model_kwargs=model_kwargs, **trainer_kwargs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e4561e29a95441ba61dc9028ab284a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/32 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc1cd52893124d36b7c0b48ff21075ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval genie_dpo-code_hard-test[:32]:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Got nans in logits 0.625\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Got nans in logits 0.625",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m _, df_raw \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;43;03m# **trainer_kwargs, \u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;43;03m# model_kwargs=model_kwargs, \u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43mscore_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscore_cls\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# __annotations__\u001b[39;00m\n\u001b[1;32m      9\u001b[0m display(_)\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/open_pref_eval/open_pref_eval/evaluation.py:153\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(datasets, trainer, model_kwargs, score_fn, **trainer_kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m get_dummy_trainer(model_kwargs\u001b[38;5;241m=\u001b[39mmodel_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrainer_kwargs)\n\u001b[0;32m--> 153\u001b[0m df_raw \u001b[38;5;241m=\u001b[39m \u001b[43meval_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscore_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# reorder df cols\u001b[39;00m\n\u001b[1;32m    156\u001b[0m cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds_i\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrect\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprob\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/open_pref_eval/open_pref_eval/evaluation.py:140\u001b[0m, in \u001b[0;36meval_datasets\u001b[0;34m(datasets, trainer, score_fn)\u001b[0m\n\u001b[1;32m    138\u001b[0m dfs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[0;32m--> 140\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43meval_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscore_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     dfs\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[1;32m    142\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(dfs)\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/open_pref_eval/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/open_pref_eval/open_pref_eval/evaluation.py:123\u001b[0m, in \u001b[0;36meval_dataset\u001b[0;34m(trainer, dataset, adapter_names, score_fn)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m adapter_name \u001b[38;5;129;01min\u001b[39;00m adapter_names:\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_adapter(trainer\u001b[38;5;241m.\u001b[39mmodel, adapter_name):\n\u001b[0;32m--> 123\u001b[0m         d \u001b[38;5;241m=\u001b[39m \u001b[43mextract_logps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscore_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m dd \u001b[38;5;129;01min\u001b[39;00m d:\n\u001b[1;32m    125\u001b[0m             dd[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madapter\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m adapter_name \u001b[38;5;28;01mif\u001b[39;00m adapter_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbase\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/open_pref_eval/open_pref_eval/evaluation.py:54\u001b[0m, in \u001b[0;36mextract_logps\u001b[0;34m(trainer, model, batch, step, score_fn)\u001b[0m\n\u001b[1;32m     52\u001b[0m bs \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchosen_input_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     53\u001b[0m i \u001b[38;5;241m=\u001b[39m bs \u001b[38;5;241m*\u001b[39m step \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(bs)\n\u001b[0;32m---> 54\u001b[0m forward_output \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenated_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m (chosen_t_logps, rejected_t_logps, chosen_mask, rejected_mask) \u001b[38;5;241m=\u001b[39m forward_output\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Here we decide how to reduce the per_token_logps to a single uncalibrated probability\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 64\u001b[0m, in \u001b[0;36mOPETrainerCls.concatenated_forward\u001b[0;34m(self, model, batch)\u001b[0m\n\u001b[1;32m     57\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m     58\u001b[0m     concatenated_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcatenated_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     59\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mconcatenated_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcatenated_attention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     60\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     63\u001b[0m nans \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39misfinite(outputs\u001b[38;5;241m.\u001b[39mlogits)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misfinite(outputs\u001b[38;5;241m.\u001b[39mlogits)\u001b[38;5;241m.\u001b[39mall(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot nans in logits \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnans\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m all_logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits[:, \u001b[38;5;241m0\u001b[39m][:, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;66;03m# convert to logits by taking last class\u001b[39;00m\n\u001b[1;32m     67\u001b[0m per_token_logps, mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_batch_logps(\n\u001b[1;32m     68\u001b[0m     logits\u001b[38;5;241m=\u001b[39mall_logits,\n\u001b[1;32m     69\u001b[0m     labels\u001b[38;5;241m=\u001b[39mconcatenated_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcatenated_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m     73\u001b[0m )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Got nans in logits 0.625"
     ]
    }
   ],
   "source": [
    "\n",
    "_, df_raw = evaluate_model(\n",
    "datasets=datasets, \n",
    "trainer=trainer,\n",
    "# **trainer_kwargs, \n",
    "# model_kwargs=model_kwargs, \n",
    "score_fn=score_cls)\n",
    "# __annotations__\n",
    "\n",
    "display(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "f_cache = Path(f\"./.cache/peft_{model_name}_results_raw.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_pref_eval.scoring import score_1st_diverg, score_cumsum, score_dpo, score_ipo, score_weighted\n",
    "\n",
    "\n",
    "if f_cache.exists() and not DEBUG:\n",
    "    print(\"Loading from cache\")\n",
    "    dfs = pickle.load(open(f_cache, 'rb'))\n",
    "else:\n",
    "    score_fns = [score_1st_diverg, score_weighted, score_cumsum, score_dpo, score_ipo]\n",
    "    dfs = {}\n",
    "    for score_fn in tqdm(score_fns, unit='score_fn'):\n",
    "\n",
    "        _, df_raw = evaluate_model(\n",
    "            datasets=datasets, \n",
    "            model=model, \n",
    "            tokenizer=tokenizer, \n",
    "            **trainer_kwargs, \n",
    "            model_kwargs=model_kwargs, \n",
    "            score_fn=score_fn)\n",
    "        _\n",
    "        dfs[score_fn.__name__] = df_raw\n",
    "\n",
    "        print(score_fn.__name__)\n",
    "        display(_)\n",
    "    pickle.dump(dfs, open(f_cache, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_cache.parent.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how much diff in terms of prob?\n",
    "# FIXME this is uncalibrated so kind of pointless\n",
    "# TODO try calibrating\n",
    "# for k, v in dfs.items():\n",
    "#     print(k)\n",
    "#     vv = v['prob']\n",
    "#     # print(vv.min(), vv.max(), vv.mean(), vv.median(), vv.std())\n",
    "#     print(f\"min={vv.min():.2f} max={vv.max():.2f} mean={vv.mean():.2f} median={vv.median():.2f} std={vv.std():.2f}\")\n",
    "#     std_by_ds = vv.reset_index().groupby('dataset').prob.mean().std()\n",
    "#     std_by_adapter =vv.reset_index().groupby('adapter').prob.mean().std()\n",
    "#     print(f\"std_by_ds: {std_by_ds:.2g}, std_by_adapter: {std_by_adapter:.2g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diff in acc? each model doesn't vary much why is that?\n",
    "for k, v in dfs.items():\n",
    "    print(k)\n",
    "    vv = v['correct']\n",
    "    print(f\"min={vv.min():.2f} max={vv.max():.2f} mean={vv.mean():.2f} median={vv.median():.2f} std={vv.std():.2f}\")\n",
    "    std_by_ds = vv.reset_index().groupby('dataset').correct.mean().std()\n",
    "    std_by_adapter =vv.reset_index().groupby('adapter').correct.mean().std()\n",
    "    print(f\"std_by_ds: {std_by_ds:.2g}, std_by_adapter: {std_by_adapter:.2g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what about per questions by model\n",
    "for k, v in dfs.items():\n",
    "    vv = v.correct.reset_index()\n",
    "    pass\n",
    "vv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot them\n",
    "for k, v in dfs.items():\n",
    "    # v.plot(kind='bar', title=k, figsize=(20, 10))\n",
    "    print(f\"# {k}\")\n",
    "    print(f\"Mean acc {v['correct'].mean():2.2g}\")\n",
    "    x = v.loc['genie_dpo-code_hard-test[:64]']['correct']\n",
    "    d = (x-x['code_easy'])['us_history_textbook']\n",
    "    print(f\"Code should be better than history at code {d:2.2g}<0?\")\n",
    "\n",
    "    x = v.loc['genie_dpo-us_history_fiction-test[:64]']['correct']\n",
    "    d = (x-x['us_history_textbook'])['code_easy']\n",
    "    print(f\"history model should be better than code model at history {d:2.2g}<0?\")\n",
    "\n",
    "    print()\n",
    "# v['correct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_pref_eval.plot.radar import radar_plot\n",
    "df_res = df_raw.groupby(['dataset', 'adapter'], dropna=False)['correct'].mean().unstack()\n",
    "radar_plot(df_res)\n",
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "# dfs = []\n",
    "# dfs_raw = []\n",
    "# for model_name in tqdm(model_names, unit='model'):\n",
    "#     adapter_name = model_name.split('-')[-1]\n",
    "#     # Hmm perhaps we need to free mem\n",
    "#     # for prev_adapter_name in model.peft_config.keys():\n",
    "#         # print(f\"Deleting {prev_adapter_name}\")\n",
    "#         # model.delete_adapter(prev_adapter_name)\n",
    "#     clear_mem()\n",
    "    \n",
    "#     model.load_adapter(model_name, adapter_name)\n",
    "#     model.set_adapter(adapter_name)\n",
    "#     # tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "#     df_agg, df_raw = evaluate_model(datasets=datasets, model=model, tokenizer=tokenizer, **trainer_kwargs, model_kwargs=model_kwargs,)\n",
    "#     df_agg['model'] = model_name\n",
    "#     df_raw['model'] = model_name\n",
    "#     dfs.append(df_agg)\n",
    "#     dfs_raw.append(df_raw)\n",
    "#     print(model_name)\n",
    "#     display(df_agg)\n",
    "# df_agg = pd.concat(dfs)\n",
    "# df_raw = pd.concat(dfs_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoPeftModelForCausalLM.from_pretrained(base_model=model, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_agg, df_raw = evaluate_models(model_names=model_names, datasets=datasets,\n",
    "                                \n",
    "#                                 # trl.DPOTrainer args\n",
    "#                                 bf16=True,\n",
    "#                                 bf16_full_eval=True,\n",
    "#                                 per_device_eval_batch_size=3,\n",
    "#                                 torch_empty_cache_steps=100,\n",
    "\n",
    "#                                 # transformers.AutoModelForCausalLM args\n",
    "#                                 model_kwargs=model_kwargs,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from tqdm.auto import tqdm\n",
    "# from open_pref_eval.evaluation import evaluate_model, get_dummy_trainer, clear_mem\n",
    "\n",
    "# def evaluate_models2(datasets, model_names, **kwargs):\n",
    "#     dfs = []\n",
    "#     dfs_raw = []\n",
    "#     for model_name in tqdm(model_names, unit='model'):\n",
    "#         model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "#         tokenizer = AutoTokenizer.from_pretrained(model_name)# TODO bnb\n",
    "#         trainer = get_dummy_trainer(model=model, tokenizer=tokenizer, **kwargs)\n",
    "#         df_agg, df_raw = evaluate_model(datasets, trainer)\n",
    "#         clear_mem()\n",
    "#         dfs.append(df_agg)\n",
    "#         dfs_raw.append(df_raw)\n",
    "#     df_agg = pd.concat(dfs)\n",
    "#     df_raw = pd.concat(dfs_raw)\n",
    "#     return df_agg, dfs_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_agg, df_raw = evaluate_models2(model_names=model_names, datasets=datasets,\n",
    "                                \n",
    "#                                 # trl args\n",
    "#                                 bf16=True,\n",
    "#                                 bf16_full_eval=True,\n",
    "#                                 per_device_eval_batch_size=2,\n",
    "#                                 torch_empty_cache_steps=100,\n",
    "# )\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_pref_eval.plot.radar import radar_plot\n",
    "df_res = df_raw.groupby(['dataset', 'adapter'], dropna=False)['correct'].mean().unstack()\n",
    "radar_plot(df_res)\n",
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res['genies-models/llama-7b-raven_matrices']\n",
    "df_res.loc['genie_dpo-raven_matrices-train'] # shouldn't the raven mode be better at raven??\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.loc['genie_dpo-us_history-train'] # should the history one be best? maybe there is a bug here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename = {}\n",
    "for row in GENIES:\n",
    "    s = 'genie_dpo-'+row['target']+'-train'\n",
    "    rename[s] = row['category']\n",
    "df_raw['category'] = df_raw.dataset.replace(rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_pref_eval.plot.radar import radar_plot\n",
    "df_res = df_raw.groupby(['category', 'model'], dropna=False)['correct'].mean().unstack()\n",
    "radar_plot(df_res)\n",
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
